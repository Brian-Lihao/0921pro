from torch.nn.parameter import UninitializedParameter 
import copy
import time
import os

import torch
from tqdm import tqdm
import numpy as np
import torch as th
import torch.nn.init as init  
from torch import nn as nn
import cv2
import pandas as pd
import matplotlib.pyplot as plt
import os.path as osp
from datetime import datetime
from contextlib import nullcontext
from pathlib import Path
import torch.nn.functional as F
from contextlib import nullcontext
import copy

from src.configs import cfg, LossDictKeys, New_DataName
from src.utils.functions import get_device, to_device, release_cuda
from src.utils.logger import TBLogger    
from src.utils.val_aug import jitter_batch  
from src.data_loader import train_eval_data_loader
from src.model.model import SwinPathBiCVAE
from src.loss import LossEvaluation
from src.backbones.dpo import DPOAligner, DPOConfig
from src.utils.teacher import get_teacher_traj

# from src.debug_viz import TBLogger, viz_fmap, paths_heatmap, plot_parp_perp, ensure_dir

class Trainer:
    def __init__(self, cfgs: cfg):
        """
        åˆå§‹åŒ–å‡½æ•°ï¼Œæ ¹æ®é…ç½®å¯¹è±¡cfgsè®¾ç½®è®­ç»ƒå’Œè¯„ä¼°æ‰€éœ€çš„å„é¡¹å‚æ•°å’Œç»„ä»¶ã€‚
        
        å‚æ•°:
        - cfgs: åŒ…å«æ‰€æœ‰é…ç½®ä¿¡æ¯çš„é…ç½®å¯¹è±¡ã€‚
        """
        # è®¾ç½®è®¾å¤‡ï¼ˆå¦‚GPUæˆ–CPUï¼‰
        self.device = get_device(device=cfgs.device)

        # æ˜¯å¦åŠ è½½å¿«ç…§å’Œè®¾ç½®æ¨¡å‹åç§°
        self.snapshot = cfgs.load_snapshot
        self.name = cfgs.name

        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        self.data_loader, self.val_loader = train_eval_data_loader(cfg=cfgs.data)
        # åˆå§‹åŒ–æ¨¡å‹å¹¶å°†å…¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.model = SwinPathBiCVAE(cfgs=cfgs.model).to(self.device)
        # self._kaiming_init() 
        self.min_of_k = int(getattr(cfgs.training,"min_of_k",1))

        # åŠ è½½è®­ç»ƒé…ç½®
        self.cfg = cfgs.training
        self.cfgs = cfgs
        self.w_eval = self.cfg.w_eval
        self.max_epoch = self.cfg.max_epoch
        self.max_iteration_per_epoch = self.cfg.max_iteration_per_epoch
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = th.optim.Adam(self.model.parameters(), lr=self.cfg.lr,weight_decay=self.cfg.weight_decay)
        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = th.optim.lr_scheduler.StepLR(self.optimizer, self.cfg.lr_decay_steps,
                                                    gamma=self.cfg.lr_decay)

        # åˆå§‹åŒ–è¿­ä»£æ¬¡æ•°å’Œçºªå…ƒæ•°
        self.iteration = 0
        self.epoch = 0
        self.training = True
        # æ¢¯åº¦ç´¯ç§¯æ­¥éª¤æ•°
        self.grad_acc_steps = self.cfg.grad_acc_steps
        # åˆå§‹åŒ–æœ€ä½³æŸå¤±ä¸ºæ— ç©·å¤§
        self.best_loss = np.inf

        # åˆå§‹åŒ–æŸå¤±å‡½æ•°å’Œè¯„ä¼°å™¨
        self.loss_func = LossEvaluation(cfg=cfgs.loss_eval).to(self.device)
        self.evaluator = LossEvaluation(cfg=cfgs.loss_eval).to(self.device)

        # ---- KL Î²-anneal åƒæ•¸ ----
        self.kld_beta_start = getattr(cfgs.loss_eval, "vae_kld_beta_start", 0.0)
        self.kld_beta_end   = getattr(cfgs.loss_eval, "vae_kld_beta_end", 1.0)
        self.kld_warmup_ep  = getattr(cfgs.loss_eval, "vae_kld_warmup_epochs", 10)

        log_dir = Path(cfgs.logger.log_name) / cfgs.name
        self.logger = TBLogger(log_dir)   
        for name, param in self.model.named_parameters():
            # â†“ åªç»™å·²åˆå§‹åŒ–å‚æ•°åŠ  hook
            if isinstance(param, UninitializedParameter):
                continue
            param.register_hook(
                lambda grad, name=name: self._grad_hook(grad, name))
            
        # add csv
        self.vis_dir   = Path(cfgs.csv_output_dir) / "val_vis"
        self.vis_dir.mkdir(parents=True, exist_ok=True)

        # === 1.1  Earlyâ€‘Stopping / Bestâ€‘Model ç›¸å…³çŠ¶æ€ ===
        self.best_sp            = float('inf')   # å½“å‰æœ€ä¼˜ split è·¯å¾„è¯¯å·®
        self.epochs_no_improve   = 0
        self.patience           = getattr(cfgs, "patience", 200)   # æ— æå‡æœ€å¤šå®¹å¿å¤šå°‘ epoch
        self.ckpt_dir           = Path(cfgs.ckpt_dir) if hasattr(cfgs, "ckpt_dir") else Path("./checkpoints")
        self.ckpt_dir.mkdir(parents=True, exist_ok=True)

        ### ONLY DEBUG
        # â”€â”€ put near other self.debug_* definitions â”€â”€
        self.debug_n_samples   = getattr(cfgs.training, "debug_n_samples", 2)     # æ¯ batch å–å‰ n
        self.debug_max_total   = getattr(cfgs.training, "debug_max_total", 50)    # å…¨ç¨‹æœ€å¤š 50 æ¡
        self._debug_saved      = 0                                                # å·²ä¿å­˜è®¡æ•°
        self.debug_txt         = (self.vis_dir / "pred_vs_gt.txt").open("a", encoding="utf-8")

        # è°ƒè¯•è¾“å‡ºè®¾ç½®ï¼ˆæ²¿ç”¨å·²æ‰©å±•çš„ TBLoggerï¼Œå¯ç›´æ¥ plot_* / add_histï¼‰
        self.debug_every_epochs = getattr(cfg, "debug_every_epochs", 1)
        self._best_epoch_cache = None  # å­˜æ”¾å½“è½®æœ€ä¼˜æ ·æœ¬çš„ä¿¡æ¯
        # --- DPO å¯¹é½å™¨ï¼ˆå¯é€‰ï¼‰ ---
        self.use_dpo = True #if self.epoch > 10 else False
        if self.use_dpo:
            dpo_cfg = DPOConfig(
                use=True,
                beta=self.cfg.dpo.beta,
                lambda_dpo=self.cfg.dpo.lambda_dpo,
                num_candidates=self.cfg.dpo.num_candidates,
                ref_update_every=self.cfg.dpo.ref_update_every,
                w_gray=self.cfg.dpo.w_gray,
                w_oob=self.cfg.dpo.w_oob,
                w_curv=self.cfg.dpo.w_curv,
                w_len=self.cfg.dpo.w_len,
                w_gt=self.cfg.dpo.w_gt,
                w_mse_energy=self.cfg.dpo.w_mse_energy,
            )
            self.dpo = DPOAligner(dpo_cfg).to(self.device)
            # å†»ç»“å‚è€ƒç­–ç•¥ï¼ˆå­¦ç”Ÿ generator çš„æ‹·è´ï¼‰
            self.ref_generator = copy.deepcopy(self.model.generator).to(self.device).eval()
            for p in self.ref_generator.parameters():
                p.requires_grad_(False)
            self._last_ref_update = 0

    def __del__(self):
        if hasattr(self, "debug_txt") and not self.debug_txt.closed:
            self.debug_txt.close()

    # --------------------------------------------------
    def _adjust_curriculum(self):
        """epochâ€‘wise åŠ¨æ€è°ƒæ•´éƒ¨åˆ† loss æƒé‡"""
        e = self.epoch
        # â€”â€” teacher forcing æ¯”ä¾‹ï¼šå‰20ep çº¿æ€§ 1.0â†’0.0ï¼Œä¹‹åæ’ 0
        if e <= 20:
            self.teacher_ratio = float(max(0.0, 1.0 - e / 20.0))
        else:
            self.teacher_ratio = 0.0
        
        if e < 20:                 # warmâ€‘up å½¢çŠ¶ä¸ºä¸»
            # self.loss_func.w_map['loss_uniform'] = 10
            self.loss_func.w_map['loss_split']  = 350
            # self.loss_func.w_map['loss_cham']    = 2
            # self.loss_func.w_map['loss_smooth']  = 10
            self.loss_func.w_map['loss_terr']    = 40
        elif e < 50:              # è¿‡æ¸¡æœŸ
            # self.loss_func.w_map['loss_uniform'] = 20
            # self.loss_func.w_map['loss_smooth']  = 30
            self.loss_func.w_map['loss_split']  = 300 
            # self.loss_func.w_map['loss_cham']    = 5
            self.loss_func.w_map['loss_terr']   = 10
        else:                     # é¿éšœä¸ºä¸»
            # self.loss_func.w_map['loss_uniform'] = 30
            # self.loss_func.w_map['loss_smooth']  = 40
            self.loss_func.w_map['loss_split']  = 200
            # self.loss_func.w_map['loss_cham']    = 10
            self.loss_func.w_map['loss_terr']    = 80

    # TODO: check later
    def save_snapshot(self, filename):
        model_state_dict = self.model.state_dict()

        # save model
        state_dict = {'model': model_state_dict}
        th.save(state_dict, filename)
        self.logger.info('Model saved to "{}"'.format(filename))

        # save snapshot
        state_dict['epoch'] = self.epoch
        state_dict['iteration'] = self.iteration
        snapshot_filename = osp.join(str(self.name) + 'snapshot.pth.tar')
        state_dict['optimizer'] = self.optimizer.state_dict()
        if self.scheduler is not None:
            state_dict['scheduler'] = self.scheduler.state_dict()
        th.save(state_dict, snapshot_filename)
        self.logger.info('Snapshot saved to "{}"'.format(snapshot_filename))

    # TODO: check later
    def load_snapshot(self, snapshot):
        self.logger.info('Loading from "{}".'.format(snapshot))
        state_dict = th.load(snapshot, map_location=th.device('cpu'))

        # Load model
        model_dict = state_dict['model']
        self.model.load_state_dict(model_dict, strict=False)

        # log missing keys and unexpected keys
        snapshot_keys = set(model_dict.keys())
        model_keys = set(self.model.state_dict().keys())
        missing_keys = model_keys - snapshot_keys
        unexpected_keys = snapshot_keys - model_keys
        if len(missing_keys) > 0:
            message = f'Missing keys: {missing_keys}'
            self.logger.error(message)
        if len(unexpected_keys) > 0:
            message = f'Unexpected keys: {unexpected_keys}'
            self.logger.error(message)
        self.logger.info('Model has been loaded.')

        # Load other attributes
        if 'epoch' in state_dict:
            self.epoch = state_dict['epoch']
            self.logger.info('Epoch has been loaded: {}.'.format(self.epoch))
        if 'iteration' in state_dict:
            self.iteration = state_dict['iteration']
            self.logger.info('Iteration has been loaded: {}.'.format(self.iteration))
        if 'optimizer' in state_dict and self.optimizer is not None:
            try:
                self.optimizer.load_state_dict(state_dict['optimizer'])
                self.logger.info('Optimizer has been loaded.')
            except:
                print("doesn't load optimizer")
        if 'scheduler' in state_dict and self.scheduler is not None:
            try:
                self.scheduler.load_state_dict(state_dict['scheduler'])
                self.logger.info('Scheduler has been loaded.')
            except:
                print("doesn't load scheduler")

    def set_train_mode(self):
        self.training = True
        self.model.train()
        th.set_grad_enabled(True)

    def set_eval_mode(self):
        self.training = False
        self.model.eval()
        th.set_grad_enabled(False)


    def optimizer_step(self, iteration):
        """æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸"""
        if iteration % self.grad_acc_steps == 0:
            scale = 1.0 / self.grad_acc_steps
            for p in self.model.parameters():
                if p.grad is not None:
                    p.grad.mul_(scale)

            # åŠ å…¥æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)  # 1.0 æ˜¯æœ€å¤§è£å‰ªå€¼

            self.optimizer.step()
            self.optimizer.zero_grad()
            
    @torch.no_grad()
    def validate_epoch(self, epoch_idx: int):
        if self.val_loader is None:
            return

        self.model.eval()
        sum_ep = sum_sp = sum_terr = sum_sm = 0.0; n_total = 0

        best_sp = float("inf"); best_pack = None   # å­˜å‚¨æœ¬è½®æœ€å¥½æ ·æœ¬
        for batch in tqdm(self.val_loader, desc=f"[VAL] {epoch_idx}", leave=False):
            if cfg.validation.enable:
                batch = jitter_batch(
                    batch,
                    max_shift = cfg.validation.pixel_shift_px,
                    n_variants= cfg.validation.n_variants,
                    n_split   = cfg.validation.split_point,
                )
            # ---------- å‰å‘ ----------
            batch = {k: v.to(self.device) if torch.is_tensor(v) else v for k, v in batch.items()}
            output = self.model(batch)
            loss_d,_ = self.loss_func(output)

            bs = output[New_DataName.y_hat].size(0)
            n_total   += bs
            sum_ep    += loss_d[LossDictKeys.loss_ep].item()     * bs
            sum_sp    += loss_d[LossDictKeys.loss_split].item()  * bs
            sum_terr  += loss_d[LossDictKeys.loss_terr].item()   * bs
            # sum_sm    += loss_d[LossDictKeys.loss_smooth].item() * bs

            # ---------- æ‹¿æœ¬ batch æœ€å° sp ä½œä¸ºå€™é€‰ ----------
            sp_sample = loss_d[LossDictKeys.loss_split].item()
            if sp_sample < best_sp:
                best_sp  = sp_sample
                best_pack = (batch, output)      # ä¿å­˜ batch+é¢„æµ‹

        # ---------- å‡å€¼ ----------
        avg_sp   = sum_sp   / n_total

        # ---------- ä¿å­˜æœ€ä½³å¯è§†åŒ– ----------
        # if best_pack is not None:
        #     b, o = best_pack
        #     img   = b[New_DataName.rgb_map][0].cpu().numpy().transpose(1,2,0)  # (128,128,3)
        #     img   = (img * 255).astype(np.uint8)
        #     wp_n  = o[New_DataName.y_hat][0].cpu().numpy()                    # (N,2)
        #     gt    = o[New_DataName.split_path][0].cpu().numpy()
        #     # ç»˜åˆ¶
        #     fig, ax = plt.subplots()
        #     ax.imshow(img); ax.plot(wp_n[:,0]*256, wp_n[:,1]*256, "r.-")
        #     ax.plot(gt[:,0]*256, gt[:,1]*256, "b.-")
        #     # plot green-start point and red-end point
        #     sx, sy = b[New_DataName.Start][0].cpu().numpy() * 256
        #     gx, gy = b[New_DataName.Goal ][0].cpu().numpy() * 256
        #     ax.plot(sx, sy, "gx"); ax.plot(gx, gy, "rx")
        #     self.total_best = loss_d[LossDictKeys.loss_total].item()
        #     # vis_path = self.vis_dir / f"epoch{epoch_idx:03d}_sp{sp_best:.3f}.png"
        #     vis_path = self.vis_dir / f"epoch{epoch_idx:03d}_total{self.total_best:.3f}.png"
        #     fig.savefig(vis_path, dpi=150); plt.close(fig)
        # åœ¨ best_pack ç¡®å®šåï¼Œé‡é‡‡ K å€™é€‰ï¼ˆæ— æ¢¯åº¦ï¼‰ï¼Œå¹¶ç¼“å­˜åˆ° _best_epoch_cache
        if best_pack is not None:
            b, o = best_pack
            with torch.no_grad():
                obs = self.model.perception(b)
                fmap = obs["fmap"]
                start = b[New_DataName.Start]; goal = b[New_DataName.Goal]
                mu, logvar = self.model.generator.encode_from_fmap(fmap, start, goal)
                std = (0.5*logvar).exp()
                K = max(4, self.min_of_k)
                eps = torch.randn((K,)+mu.shape, device=mu.device)
                cands = []
                for k in range(K):
                    z = mu + eps[k] * std
                    y = self.model.generator.decode(z, fmap, start, goal)   # [B,N,2]
                    cands.append(y)
                C = torch.stack(cands, dim=0)  # [K,B,N,2]

                b0 = 0
                self._best_epoch_cache = dict(
                    fmap=fmap[b0:b0+1].cpu(),
                    raw=b[New_DataName.terrain_cost_map][b0:b0+1].cpu(),   # åŸç°åº¦å›¾
                    gt=b[New_DataName.split_path][b0].cpu(),
                    pred=o[New_DataName.y_hat][b0].cpu(),
                    cands=C[:, b0].cpu(),
                    H=fmap.size(-2), W=fmap.size(-1),
                    parp=None, perp=None, gate_t=None,           # â† å ä½ï¼Œé¿å… KeyError
                    mu=mu.detach().cpu(), std=std.detach().cpu()
                )
                img   = b[New_DataName.rgb_map][0].cpu().numpy().transpose(1,2,0)  # (128,128,3)
                img   = (img * 255).astype(np.uint8)
                wp_n  = o[New_DataName.y_hat][0].cpu().numpy()                    # (N,2)
                gt    = o[New_DataName.split_path][0].cpu().numpy()
                fig, ax = plt.subplots()
                ax.imshow(img); ax.plot(wp_n[:,0]*256, wp_n[:,1]*256, "r.-")
                ax.plot(gt[:,0]*256, gt[:,1]*256, "b.-")
                # plot green-start point and red-end point
                sx, sy = b[New_DataName.Start][0].cpu().numpy() * 256
                gx, gy = b[New_DataName.Goal ][0].cpu().numpy() * 256
                ax.plot(sx, sy, "gx"); ax.plot(gx, gy, "rx")
                self.total_best = loss_d[LossDictKeys.loss_total].item()
                # vis_path = self.vis_dir / f"epoch{epoch_idx:03d}_sp{sp_best:.3f}.png"
                vis_path = self.vis_dir / f"epoch{epoch_idx:03d}_total{self.total_best:.3f}.png"
                fig.savefig(vis_path, dpi=150); plt.close(fig)


        if avg_sp < self.best_sp:
            # improved = True
            self.best_sp          = avg_sp
            self.epochs_no_improve = 0
            ckpt_path = self.ckpt_dir / f"best_ep{epoch_idx:03d}_sp{avg_sp:.4f}.pth"
            torch.save(self.model.state_dict(), ckpt_path)#"best_model.pth")
            print(f"[VAL] âœ…  New best sp={avg_sp:.4f}, model saved to {ckpt_path}")
        else:
            self.epochs_no_improve += 1
            print(f"[VAL] ğŸ”¸  Now: {avg_sp:.4f} No improv. best_sp={self.best_sp:.4f} â–¸ {self.epochs_no_improve}/{self.patience}")

        self.model.train()
        return best_pack

    def step(self, data_dict):
        """
        æ‰§è¡Œå•æ¬¡è®­ç»ƒæˆ–æ¨ç†æ­¥éª¤ã€‚

        å‚æ•°:
        - data_dict (dict): åŒ…å«è¾“å…¥æ•°æ®çš„å­—å…¸ï¼Œé€šå¸¸åŒ…æ‹¬ç‰¹å¾å’Œæ ‡ç­¾ç­‰ä¿¡æ¯ã€‚

        è¿”å›:
        - output_dict (dict): æ¨¡å‹çš„è¾“å‡ºç»“æœã€‚
        - loss_dict (dict): è®¡ç®—å¾—åˆ°çš„æŸå¤±å€¼å­—å…¸ã€‚
        """
        # å°†æ•°æ®å­—å…¸ä¸­çš„æ‰€æœ‰å¼ é‡ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå¦‚GPUï¼‰
        data_dict = to_device(data_dict, device=self.device)
        # ç¡®ä¿ terrain_cost_map çš„å½¢çŠ¶æ˜¯ (B, W, H)
        # if New_DataName.rgb_map in data_dict and data_dict[New_DataName.rgb_map].dim() == 4:
        #     data_dict[New_DataName.rgb_map] = data_dict[New_DataName.rgb_map].squeeze(1)
        
        # ç¡®ä¿ last_poses æ˜¯å¼ é‡
        if isinstance(data_dict.get(New_DataName.last_poses, None), list):
            data_dict[New_DataName.last_poses] = torch.stack([torch.tensor(p) for p in data_dict[New_DataName.last_poses]]).to(self.device)
        elif data_dict.get(New_DataName.last_poses, None) is not None:
            data_dict[New_DataName.last_poses] = data_dict[New_DataName.last_poses].clone().detach().to(self.device)
        
        # # ä½¿ç”¨æ¨¡å‹å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè·å–è¾“å‡ºç»“æœ
        # output_dict = self.model(data_dict)

        # # ä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—è¾“å‡ºç»“æœå¯¹åº”çš„æŸå¤±å€¼
        # loss_dict, extra_dict = self.loss_func(output_dict)
        # ========== Min-of-Kï¼ˆBest-of-Kï¼‰ ==========
        # æ€è·¯ï¼šå‰å‘å–æ¨£ K æ¬¡ â†’ ç”¨èˆ‡ GT çš„ MSE ä½œç‚ºä¾¿å®œåˆ†æ•¸ â†’ å°æ¯å€‹ batch item é¸å‡ºæœ€ä½³å€™é¸
        #  # ç„¶å¾Œåªåœ¨ "æœ€ä½³å€™é¸" ä¸Šè¨ˆç®—å®Œæ•´ä»»å‹™æå¤±ä¸¦åå‚³
        # ========= Memory-Efficient Min-of-K =========
        K = self.min_of_k if self.model.training else 1 # é©—è­‰éšæ®µå¦‚éœ€è§€å¯Ÿè¦†è“‹ï¼Œå¯æŠŠé€™è£¡æ”¹æˆ self.min_of_k
        # print(f"value of k:{K}")
        if (not self.model.training) or (K <= 1):
            # å¸¸è¦è·¯å¾‘
            output_dict = self.model(data_dict)
            loss_dict, extra_dict = self.loss_func(output_dict)
        else:
            # ---- 1) ç„¡æ¢¯åº¦çš„ã€Œå€™é¸æ‰“åˆ†ã€éšæ®µï¼šåªç‚ºé¸ zï¼Œä¸ç•™åœ– ----
            start = data_dict[New_DataName.Start]
            goal  = data_dict[New_DataName.Goal]
            split_gt = data_dict[New_DataName.split_path]        # (B,N,2)
            with torch.no_grad():
                # åªç®—ä¸€æ¬¡æ„ŸçŸ¥ç·¨ç¢¼
                obs = self.model.perception(data_dict)           # {"fmap","gvec"}

                fmap = obs["fmap"]
                # ç»Ÿä¸€ï¼šæ”¹ç”¨ 3Ã—3 åŒºåŸŸæ± åŒ–çš„ç¼–ç å…¥å£ï¼Œé¿å…ç»´åº¦ 196/1732 ä¸åŒ¹é…
                mu, logvar = self.model.generator.encode_from_fmap(fmap, start, goal)

                std = (0.5 * logvar).exp()
                # K å€‹ epsï¼Œå…ˆæŠ½å¥½ä»¥ä¾¿ç¨å¾Œå¾©ç¾åŒä¸€å€‹æœ€ä½³ z
                eps = torch.randn((K,)+mu.shape, device=mu.device)
                # é€å€‹ z è§£ç¢¼ â†’ è·¯å¾‘ â†’ MSE æ‰“åˆ†ï¼ˆçœé¡¯å­˜ã€ä¸ç•™åœ–ï¼‰
                cand_paths = []
                for k in range(K):
                    z_k = mu + eps[k] * std
                    # y_k = self.model.generator.decode(z_k, fmap, start, goal)   # (B,N,2)
                    y_k = self.model.generator.decode(
                        z_k, fmap, start, goal,
                        gt=split_gt, teacher_ratio=self.teacher_ratio
                    )   # (B,N,2)
                    cand_paths.append(y_k)

                Y = torch.stack(cand_paths, dim=0)                              # (K,B,N,2)

                mse_coord = ((Y - split_gt.unsqueeze(0))**2).mean(dim=(2,3))
                Vp = Y[:, :, 1:, :] - Y[:, :, :-1, :]
                Vg = (split_gt[:, 1:, :] - split_gt[:, :-1, :]).unsqueeze(0)
                mse_vec = (Vp - Vg).pow(2).mean(dim=(2,3))
                Cp = Y[:, :, 2:, :] - 2*Y[:, :, 1:-1, :] + Y[:, :, :-2, :]
                Cg = (split_gt[:, 2:, :] - 2*split_gt[:, 1:-1, :] + split_gt[:, :-2, :]).unsqueeze(0)
                mse_curv = (Cp - Cg).pow(2).mean(dim=(2,3))
                # æœ€ç»ˆæ‰“åˆ†ï¼ˆå¯å¾®è°ƒï¼šç›´çº¿åŒ–æ—¶æé«˜æ›²ç‡æƒé‡ï¼‰

                score = 0.5 * mse_coord + 0.3 * mse_vec + 0.2 * mse_curv        # (K,B)
                best_idx = torch.argmin(score, dim=0)                            # (B,)
                # å–æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æœ€ä½³ epsï¼Œåé¢å¸¦æ¢¯åº¦é‡ç®—ä¸€æ¬¡
                # best_idx = torch.argmin(score, dim=0)  
                # y_best = self.model.generator.decode(
                #     z_best, fmap, start, goal,
                #     gt=split_gt, teacher_ratio=self.teacher_ratio
                # )
                # å–æ¯å€‹æ¨£æœ¬å°æ‡‰çš„æœ€ä½³ epsï¼Œå¾Œé¢å¸¶æ¢¯åº¦é‡ç®—ä¸€æ¬¡
                B = mu.size(0)
                ar = torch.arange(B, device=mu.device)
                best_eps = eps[best_idx, ar, ...].detach()                      # (B,zd)

            # ---- 2) æ­£å¼å¸¶æ¢¯åº¦çš„ã€Œå–®æ¬¡å‰å‘ã€ï¼šé‡ç®—åŒä¸€å€‹æœ€ä½³ z ----
            # é‡æ–°ç®—ä¸€éæ„ŸçŸ¥èˆ‡ encodeï¼ˆå¸¶æ¢¯åº¦ï¼‰
            obs = self.model.perception(data_dict)

            fmap = obs["fmap"]
            # åŒæ ·è¿™é‡Œä¹Ÿä½¿ç”¨ç»Ÿä¸€å…¥å£
            mu, logvar = self.model.generator.encode_from_fmap(fmap, start, goal)

            std = (0.5 * logvar).exp()
            # z_best = mu + best_eps * std                                        # æ¢¯åº¦åƒ…æµå‘ Î¼,logÏƒ
            # y_best = self.model.generator.decode(z_best, fmap, start, goal)
            z_best = mu + best_eps * std                                        # æ¢¯åº¦ä»…æµå‘ Î¼,logÏƒ
            y_best = self.model.generator.decode(
                z_best, fmap, start, goal, gt=split_gt, teacher_ratio=self.teacher_ratio
            )
            # â€”â€” è®°å½•å½“è½®â€œæœ€ä¼˜æ ·æœ¬â€ç”¨äºè°ƒè¯• â€”â€” 
            with torch.no_grad():
                b0 = 0
                y_dbg, dbg = self.model.generator.decode(z_best, fmap, start, goal, return_debug=True)
                H, W = fmap.shape[-2], fmap.shape[-1]
                self._best_epoch_cache = dict(
                    loss=None,
                    fmap=fmap[b0:b0+1].detach().cpu(),
                    gt=split_gt[b0].detach().cpu(),
                    pred=y_dbg[b0].detach().cpu(),
                    cands=Y[:, b0].detach().cpu(),
                    H=H, W=W,
                    parp=(dbg.get("parp")[b0] if dbg.get("parp") is not None else None),
                    perp=(dbg.get("perp")[b0] if dbg.get("perp") is not None else None),
                    gate_t=(dbg.get("gate_t")[b0] if dbg.get("gate_t") is not None else None),
                    mu=mu.detach().cpu(), std=std.detach().cpu(),
                    score_coord=mse_coord[:, b0].detach().cpu(),
                    score_vec=mse_vec[:, b0].detach().cpu(),
                    score_curv=mse_curv[:, b0].detach().cpu()
                )
               


            # çµ„è£ output_dictï¼ˆæ²¿ç”¨æ¨¡å‹ forward çš„éµä½ï¼Œæ–¹ä¾¿åŸ loss ä½¿ç”¨ï¼‰
            output_dict = {
                New_DataName.y_hat: y_best,
                New_DataName.mu: mu,
                New_DataName.logvar: logvar,
                # ä»¥ä¸‹å¾ batch ç›´æ¥å¸¶éå»
                New_DataName.Start: data_dict[New_DataName.Start],
                New_DataName.Goal:  data_dict[New_DataName.Goal],
                New_DataName.terrain_cost_map: data_dict[New_DataName.terrain_cost_map],
                New_DataName.split_path: split_gt,
            }
            loss_dict, extra_dict = self.loss_func(output_dict)

        # === DPO åå¥½å¾®è°ƒï¼ˆå¯é€‰ï¼ŒåŠ åˆ° total_loss ä¸Šï¼‰ ===
        if self.use_dpo:
            # å‚è€ƒç­–ç•¥å‘¨æœŸæ€§æ›´æ–°ï¼ˆEMA/æ‹·è´å½“å‰æƒé‡ï¼‰
            self._last_ref_update += 1
            if self._last_ref_update >= self.cfg.dpo.ref_update_every:
                self.ref_generator.load_state_dict(self.model.generator.state_dict())
                self.ref_generator.eval()
                self._last_ref_update = 0

            # å‡†å¤‡è¾“å…¥
            observation = self.model.perception(data_dict)
            start = data_dict[New_DataName.Start]
            goal  = data_dict[New_DataName.Goal]
            gt_traj = get_teacher_traj(data_dict)  # T0: ç›´æ¥ç”¨æ•°æ®é›† GT ä½œä¸ºè€å¸ˆ

            dpo_out = self.dpo(
                model=self.model.generator,
                ref_model=self.ref_generator,
                input_dict=data_dict,
                observation=observation,
                start=start, goal=goal,
                gt=gt_traj,
                num_candidates=self.cfg.dpo.num_candidates
            )
            loss_dpo = dpo_out["loss_dpo"] * self.cfg.dpo.lambda_dpo
            # total_loss = loss_dict[] + loss_dpo

            loss_dict[LossDictKeys.loss_total] = loss_dict[LossDictKeys.loss_total] + loss_dpo

            
        return output_dict, loss_dict, extra_dict

    def update_logger(self, result_dict: dict):
        for key, value in result_dict.items():
            self.logger.record("train/{}".format(key), value=value)

 
    def run_epoch(self):
        """
        æ‰§è¡Œä¸€ä¸ªè®­ç»ƒå‘¨æœŸã€‚
        
        æœ¬å‡½æ•°å°†å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        - æ¸…é›¶æ¢¯åº¦
        - é‡ç½®æ•°æ®åŠ è½½å™¨ï¼ˆå·²æ³¨é‡Šï¼‰
        - è®°å½•å‘¨æœŸå¼€å§‹æ—¶é—´
        - éå†æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
        - æ ¹æ®æœ€å¤§è¿­ä»£æ¬¡æ•°é™åˆ¶æ¯ä¸ªå‘¨æœŸçš„è¿­ä»£æ•°
        - æ‰§è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
        - ä¼˜åŒ–æ¨¡å‹å‚æ•°global_step
        - æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        - ä¿å­˜æ¨¡å‹å¿«ç…§
        """

        self._adjust_curriculum()
        self.optimizer.zero_grad()
        ctx = torch.autograd.detect_anomaly if self.cfg.debug_anomaly else nullcontext

        # æ¯è½®åˆå§‹åŒ–â€œæœ€ä½³æ ·æœ¬â€ç¼“å­˜
        self._best_epoch_cache = dict(loss=float("inf"))

        dl_iter = tqdm(
            enumerate(self.data_loader),
            total=len(self.data_loader),
            desc=f"Epoch {self.epoch}"
        )

        for iteration, data_dict in dl_iter:

            if iteration and iteration % self.max_iteration_per_epoch == 0:
                break
            # ---- è¨­ç½®ç•¶å‰ epoch çš„ KL Î² ----
            if self.kld_warmup_ep > 0:
                t = min(1.0, float(self.epoch) / float(self.kld_warmup_ep))
            else:
                t = 1.0
            cur_beta = self.kld_beta_start + (self.kld_beta_end - self.kld_beta_start) * t
            # å‚³çµ¦ loss æ¨¡çµ„
            if hasattr(self.loss_func, "kld_beta"):
                self.loss_func.kld_beta = float(cur_beta)
            self.iteration += 1
            # output_dict, result_dict = self.step(data_dict)
            output_dict, result_dict, extra_dict = self.step(data_dict)

            loss = result_dict[LossDictKeys.loss]

            # ==== tqdm å®æ—¶ä¿¡æ¯ ====
            postfix = {
                "loss":      f"{result_dict[LossDictKeys.loss_total].item():.3f}",
                # "ep":        f"{result_dict[LossDictKeys.loss_ep].item():.3f}",
                "oob":       f"{result_dict[LossDictKeys.loss_oob].item():.3f}",
                # "unif":  f"{result_dict[LossDictKeys.loss_uniform].item():.3f}",
                # "terr":      f"{result_dict[LossDictKeys.loss_terr].item():.3f}",
                # "sm":        f"{result_dict[LossDictKeys.loss_smooth].item():.3f}",
                "sp":        f"{result_dict[LossDictKeys.loss_split].item():.3f}",
                # "jump":      f"{result_dict[LossDictKeys.loss_jump].item():.3f}",
                "kl":        f"{result_dict.get(LossDictKeys.vae_kld_loss, 0.0):.3f}",
                "Î²kl":       f"{float(cur_beta):.2f}",
                # "cos":       f"{result_dict[LossDictKeys.loss_cos].item():.3f}",
                "|Å·|":       f"{output_dict[New_DataName.y_hat].norm(dim=-1).mean().item():.3f}",
                "lr":        f"{self.scheduler.get_last_lr()[0]:.3e}",  # å¦‚æœéœ€è¦ç§‘å­¦è®¡æ•°æ³•çš„å°æ•°ç‚¹åä¸‰ä½
            }
            dl_iter.set_postfix(postfix)

            # ==== NaN æ£€æŸ¥ ====
            if torch.isnan(loss):
                tqdm.write("âš  NaN detected â€” skip batch")
                continue

            # ==== backward ====
            with ctx():
                loss.backward()

            ### new logger
            global_step = self.epoch * len(self.data_loader) + iteration

            # â‘  å…¨éƒ¨ raw loss
            raw_losses = {k: v.item() for k, v in result_dict.items() if "loss_" in k}

            # â‘¡ â€œraw Ã— æƒé‡â€  (è·³è¿‡ w_map ä¸­æ²¡æœ‰çš„é”®ï¼Œä¾‹å¦‚ loss_total)
            contrib = {f"{k}_c": raw_losses[k] * self.loss_func.w_map.get(k, 1.0)
                    for k in raw_losses if k in self.loss_func.w_map}

            # â‘¢ å†™ TensorBoard / CSV
            # self.logger.add_scalar("train_raw",     raw_losses, global_step)
            # self.logger.add_scalar("train_contrib", contrib,    global_step)
            # if extra_dict:
            #     self.logger.add_scalar("train_extra", extra_dict, global_step)
            # â‘¢ å†™ TensorBoard / CSVï¼ˆå­—å…¸å¿…é¡»ç”¨ add_scalarsï¼‰
            self.logger.add_scalars("train_raw",     raw_losses, global_step)
            self.logger.add_scalars("train_contrib", contrib,    global_step)
            if extra_dict:
                # å°† extra_dict é‡Œå¯èƒ½çš„ tensor/æ•°ç»„è½¬ä¸º float
                extras = {}
                for k, v in extra_dict.items():
                    if torch.is_tensor(v):
                        extras[k] = v.detach().float().mean().item()
                    else:
                        try: extras[k] = float(v)
                        except: continue
                if len(extras) > 0:
                    self.logger.add_scalars("train_extra", extras, global_step)

            # æ¢¯åº¦æ¸…ç†
            for p in self.model.parameters():
                if p.grad is not None:
                    torch.nan_to_num_(p.grad, nan=0.0, posinf=1e4, neginf=-1e4)

            self.optimizer_step(iteration + 1)
            
            # å°†ç»“æœå­—å…¸ä¸­çš„æ•°æ®ä»GPUè½¬ç§»åˆ°CPUï¼Œä»¥é‡Šæ”¾æ˜¾å­˜ç©ºé—´
            result_dict = release_cuda(result_dict)

            # å†æ¬¡é‡Šæ”¾ç¼“å­˜ï¼Œä»¥èŠ‚çœæ˜¾å­˜ç©ºé—´
            th.cuda.empty_cache()
        
        # å¦‚æœå­¦ä¹ ç‡è°ƒåº¦å™¨ä¸ä¸ºç©ºï¼Œåˆ™æ›´æ–°å­¦ä¹ ç‡
        if self.scheduler is not None:
            self.scheduler.step()
        
        # åˆ›å»ºä¿å­˜æ¨¡å‹çš„ç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        os.makedirs("./models/{}".format(self.name), exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹å¿«ç…§
        self.save_snapshot(f'models/{self.name}/last.pth.tar')
        
    def _grad_hook(self, grad, name):
        """æ¢¯åº¦å¼‚å¸¸æ£€æµ‹é’©å­"""
        if grad is None:
            return
        if torch.isnan(grad).any():
            print(f"[!] NaN gradient detected in {name}")
        if torch.isinf(grad).any():
            print(f"[!] Inf gradient detected in {name}")

    def _write_png(self, terrain_cost_map=None, center=None, targets=None, paths=None, path=None,
                   others=None, file="test_terrain_cost_map.png"):
        dis = 2
        if len(terrain_cost_map.shape) == 2:
            terrain_cost_map_fig = np.repeat(terrain_cost_map[:, :, np.newaxis], 3, axis=2) * 255
        else:
            terrain_cost_map_fig = copy.deepcopy(terrain_cost_map)

            for path in paths:
                if len(path) == 1 or np.any(path[0] == np.inf):
                    continue
                path = np.asarray(path, dtype=int)
                assert path.shape[1] == 2 and len(path.shape) == 2 and path.shape[0] >= 2, "path should be Nx2"
                all_pts = np.concatenate((path + np.array([0, -1], dtype=int), path + np.array([1, 0], dtype=int),
                                          path + np.array([-1, 0], dtype=int), path + np.array([0, 1], dtype=int),
                                          path), axis=0)
                all_pts = np.clip(all_pts, 0, terrain_cost_map_fig.shape[0] - 1)
                terrain_cost_map_fig[all_pts[:, 0], all_pts[:, 1], 0] = 255
                terrain_cost_map_fig[all_pts[:, 0], all_pts[:, 1], 1] = 0
                terrain_cost_map_fig[all_pts[:, 0], all_pts[:, 1], 2] = 255
        cv2.imwrite(file, terrain_cost_map_fig)
        return terrain_cost_map_fig

    def _display_output(self, output_dict, data_dict, iteration, terrain_cost_map_resolution=0.1, terrain_cost_map_threshold=300,
                        root_path="training"):
        if not os.path.exists(root_path):
            os.makedirs(root_path)
        y_hat = output_dict[New_DataName.y_hat]  # BxNx2
        for i in range(len(y_hat)):
            file_name = str(int(data_dict[New_DataName.terrain_cost_map][i].detach().cpu().numpy()[0])) + ".png"
            terrain_cost_map = cv2.imread(os.path.join(self.data_loader.dataset.root, self.data_loader.dataset.figures, file_name))
            terrain_cost_map = terrain_cost_map[int(terrain_cost_map.shape[0] / 2 - terrain_cost_map_threshold):
                                  int(terrain_cost_map.shape[0] / 2 + terrain_cost_map_threshold),
                        int(terrain_cost_map.shape[1] / 2 - terrain_cost_map_threshold):
                        int(terrain_cost_map.shape[1] / 2 + terrain_cost_map_threshold)]
            center = np.array([terrain_cost_map.shape[0] / 2.0, terrain_cost_map.shape[1] / 2.0], dtype=int)

            scan_pixels = np.clip(np.floor(data_dict[New_DataName.scan][i].detach().cpu().numpy()[:, :2] /
                                           terrain_cost_map_threshold).astype(int) + center, 0, terrain_cost_map.shape[0] - 1)
            if len(y_hat.shape) == 3:
                points = torch.cumsum(y_hat[i], dim=0).detach().cpu().numpy()
                pixels = np.clip(np.floor(points / terrain_cost_map_resolution).astype(int) + center, 0,
                                 terrain_cost_map.shape[0] - 1)
                self._write_png(terrain_cost_map=terrain_cost_map, center=center, path=pixels, others=scan_pixels,
                                file=os.path.join(root_path, "evaluation_{}_{}_{}.png".format(self.epoch, iteration, i)))
            else:
                points = torch.cumsum(y_hat[i], dim=1).detach().cpu().numpy()
                pixels = np.clip(np.floor(points / terrain_cost_map_resolution).astype(int) + center, 0,
                                 terrain_cost_map.shape[0] - 1)
                self._write_png(terrain_cost_map=terrain_cost_map, center=center, paths=pixels, others=scan_pixels,
                                file=os.path.join(root_path, "evaluation_{}_{}_{}.png".format(self.epoch, iteration, i)))

    def inference_epoch(self):
        """
        æ‰§è¡Œä¸€ä¸ªéªŒè¯è½®æ¬¡çš„æ¨ç†ã€‚
        è¯¥æ–¹æ³•ç”¨äºåœ¨å½“å‰epochç»“æŸåï¼Œå¯¹éªŒè¯æ•°æ®é›†è¿›è¡Œæ¨ç†ï¼Œè®°å½•æ¨ç†ç»“æœï¼Œå¹¶æœ€ç»ˆæ¢å¤è®­ç»ƒæ¨¡å¼ã€‚
        """
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œä»¥ä¾¿åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¦ç”¨dropoutç­‰è®­ç»ƒæ—¶çš„ç‰¹æ€§
        self.set_eval_mode()
        
        # éå†éªŒè¯æ•°æ®åŠ è½½å™¨æä¾›çš„æ¯ä¸ªæ•°æ®æ‰¹æ¬¡
        for iteration, data_dict in enumerate(self.val_loader):

            # æ‰§è¡Œæ¨ç†æ­¥éª¤ï¼Œè·å–è¾“å‡ºå’Œç»“æœå­—å…¸
            output_dict, result_dict = self.step(data_dict)
            # æ¯20ä¸ªæ‰¹æ¬¡å¯è§†åŒ–ä¸€æ¬¡è¾“å‡ºï¼Œä»¥ä¾¿æ£€æŸ¥æ¨ç†ç»“æœ
            if iteration % 20 == 0:
                self._display_output(output_dict=output_dict, data_dict=data_dict, iteration=iteration,
                                    root_path="training/"+self.name)
            # ç¡®ä¿æ‰€æœ‰CUDAæ“ä½œå®Œæˆ
            th.cuda.synchronize()

            # é‡Šæ”¾CUDAå¼ é‡ï¼Œé¿å…å†…å­˜æ³„æ¼
            result_dict = release_cuda(result_dict)

            # æ¸…ç©ºCUDAç¼“å­˜ï¼Œé‡Šæ”¾ä¸å¿…è¦çš„å†…å­˜
            th.cuda.empty_cache()

        self.set_train_mode()

    def run(self):
        """
        å¯åŠ¨è®­ç»ƒè¿‡ç¨‹ã€‚
        
        æœ¬å‡½æ•°é¦–å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨å¿«ç…§ï¼Œå¦‚æœå­˜åœ¨ï¼Œåˆ™åŠ è½½å¿«ç…§ã€‚
        ç„¶åè®¾ç½®è®­ç»ƒæ¨¡å¼ï¼Œå¹¶å¼€å§‹è®­ç»ƒå¾ªç¯ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§è®­ç»ƒå‘¨æœŸæ•°ã€‚
        åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¹‹åï¼Œå¦‚æœè®¾ç½®äº†è¯„ä¼°é—´éš”ï¼Œåˆ™è¿›è¡Œä¸€æ¬¡æ¨ç†å‘¨æœŸã€‚
        """
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¿«ç…§ï¼Œå¦‚æœå­˜åœ¨åˆ™åŠ è½½
        if self.snapshot:
            self.load_snapshot(self.snapshot)

        # è®¾ç½®è®­ç»ƒæ¨¡å¼
        self.set_train_mode()

        # print all loss weights setting
        for key, value in self.loss_func.w_map.items():
            print(f"loss setting {key}: {value}")
        
        # è®­ç»ƒå¾ªç¯ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§è®­ç»ƒå‘¨æœŸæ•°
        while self.epoch < self.max_epoch:
            self.epoch += 1
            self.run_epoch()
            best_pack = self.validate_epoch(self.epoch)                # ç«‹å³è·‘éªŒè¯
            if self.epoch % self.debug_every_epochs == 0 and self._best_epoch_cache:
                # 1) fmapï¼ˆé€šé“æ‹¼è´´ + å‡å€¼ï¼‰
                self.logger.plot_fmap(self._best_epoch_cache["fmap"][0], step=self.epoch, tag_prefix="debug/fmap")
                # 2) å€™é€‰è·¯å¾„çƒ­å›¾ï¼ˆèƒŒæ™¯ç”¨ fmap å‡å€¼ï¼‰
                bg = self._best_epoch_cache["fmap"][0].mean(0)
                # self.logger.plot_paths_heatmap(self._best_epoch_cache["cands"],
                #                                self._best_epoch_cache["pred"],
                #                                self._best_epoch_cache["gt"],
                #                                self._best_epoch_cache["H"], self._best_epoch_cache["W"],
                #                                step=self.epoch, tag_prefix="debug/paths", bg=bg)
                self.logger.plot_paths_heatmap(self._best_epoch_cache["cands"],
                               self._best_epoch_cache["pred"],
                               self._best_epoch_cache["gt"],
                               self._best_epoch_cache["H"], self._best_epoch_cache["W"],
                               step=self.epoch, tag_prefix="debug/paths", bg=bg)
                # # ç›´æ–¹
                # # coord = ((self._best_epoch_cache["cands"] - self._best_epoch_cache["gt"])**2).mean(dim=(2,3))
                # C   = self._best_epoch_cache["cands"]         # [K,N,2] æˆ– [K,B,N,2]
                # GT  = self._best_epoch_cache["gt"]            # [N,2]
                # # ç»Ÿä¸€åˆ° [K,?,N,2] çš„æœ€åä¸¤ç»´åšå‡å€¼
                # if C.dim() == 3:   # [K,N,2]
                #     coord = ((C - GT.unsqueeze(0))**2).mean(dim=(1,2))  # â†’ [K]
                # else:               # [K,B,N,2]ï¼ˆæ›´é€šç”¨çš„å…œåº•ï¼‰
                #     # ä½ è¿™é‡Œå­˜çš„æ˜¯å•æ ·æœ¬ b0ï¼Œä»å¯åš (2,3) å¹³å‡ï¼Œå†åœ¨ B ç»´ä¸Šå–å‡å€¼
                #     coord = ((C - GT.unsqueeze(0).unsqueeze(1))**2).mean(dim=(2,3)).mean(dim=1)  # â†’ [K]
                # # self.logger.add_hist("score/coord", coord, self.epoch)
                # self.logger.add_hist("score/coord", coord, self.epoch)
                # # 3) parp/perp æ›²çº¿ + ç›´æ–¹å›¾
                # if self._best_epoch_cache["parp"] is not None and self._best_epoch_cache["perp"] is not None:
                #     self.logger.plot_parp_perp(self._best_epoch_cache["parp"],
                #                                self._best_epoch_cache["perp"],
                #                                step=self.epoch, tag_prefix="debug/step")
                # # 4) å€™é€‰æ‰“åˆ†ç›´æ–¹å›¾
                # if self._best_epoch_cache["score_coord"] is not None:
                #     self.logger.add_hist("score/coord", self._best_epoch_cache["score_coord"], self.epoch)
                # if self._best_epoch_cache["score_vec"] is not None:
                #     self.logger.add_hist("score/vec",   self._best_epoch_cache["score_vec"],   self.epoch)
                # if self._best_epoch_cache["score_curv"] is not None:
                #     self.logger.add_hist("score/curv",  self._best_epoch_cache["score_curv"],  self.epoch)
                # === paths heatmap stays unchanged above ===

                C  = self._best_epoch_cache["cands"]   # [K,N,2] or [K,B,N,2]
                GT = self._best_epoch_cache["gt"]      # [N,2]

                # coord (pointwise MSE)
                if C.dim() == 3:  # [K,N,2]
                    score_coord = ((C - GT.unsqueeze(0))**2).mean(dim=(1,2))           # [K]
                else:             # [K,B,N,2]
                    score_coord = ((C - GT.unsqueeze(0).unsqueeze(1))**2).mean(dim=(2,3)).mean(dim=1)

                # vec (first-diff MSE)
                if C.dim() == 3:
                    Vp = C[:, 1:, :] - C[:, :-1, :]
                    Vg = GT[1:, :] - GT[:-1, :]
                    score_vec = ((Vp - Vg.unsqueeze(0))**2).mean(dim=(1,2))
                else:
                    Vp = C[:, :, 1:, :] - C[:, :, :-1, :]
                    Vg = (GT[1:, :] - GT[:-1, :]).unsqueeze(0).unsqueeze(1)
                    score_vec = ((Vp - Vg)**2).mean(dim=(2,3)).mean(dim=1)

                # curv (second-diff MSE)
                if C.dim() == 3:
                    Cp = C[:, 2:, :] - 2*C[:, 1:-1, :] + C[:, :-2, :]
                    Cg = GT[2:, :] - 2*GT[1:-1, :] + GT[:-2, :]
                    score_curv = ((Cp - Cg.unsqueeze(0))**2).mean(dim=(1,2))
                else:
                    Cp = C[:, :, 2:, :] - 2*C[:, :, 1:-1, :] + C[:, :, :-2, :]
                    Cg = (GT[2:, :] - 2*GT[1:-1, :] + GT[:-2, :]).unsqueeze(0).unsqueeze(1)
                    score_curv = ((Cp - Cg)**2).mean(dim=(2,3)).mean(dim=1)

                # write histograms
                self.logger.add_hist("score/coord", score_coord, self.epoch)
                self.logger.add_hist("score/vec",   score_vec,   self.epoch)
                self.logger.add_hist("score/curv",  score_curv,  self.epoch)
                parp = self._best_epoch_cache.get("parp", None)
                perp = self._best_epoch_cache.get("perp", None)
                if (parp is not None) and (perp is not None):
                    self.logger.plot_parp_perp(parp, perp, step=self.epoch, tag_prefix="debug/step")
                # 5) KL / z ç»Ÿè®¡
                kl = (-0.5 * (1 + 2*torch.log(self._best_epoch_cache["std"]) - self._best_epoch_cache["mu"]**2 - (self._best_epoch_cache["std"]**2))).sum(dim=1).mean()
                self.logger.add_scalars("latent", {"KL_mean": float(kl.detach().cpu().item())}, self.epoch)
                if self._best_epoch_cache.get("loss") is not None:
                    self.logger.add_scalars("train", {"best_loss": self._best_epoch_cache["loss"]}, self.epoch)
            
            # ---- Earlyâ€‘Stopping ----
            if self.epochs_no_improve >= self.patience:
                print(f"Early Stopping: no sp improvement for {self.patience} epochs")
                break

if __name__ == "__main__":
    trainer = Trainer(cfgs=cfg)
    trainer.run_epoch()
